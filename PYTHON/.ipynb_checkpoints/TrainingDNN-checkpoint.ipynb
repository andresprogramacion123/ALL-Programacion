{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ENTRENANDO REDES NEURONALES PROFUNDAS (DEEP LEARNING) </H1>\n",
    "<IMG SRC=\"https://e00-elmundo.uecdn.es/assets/multimedia/imagenes/2018/02/15/15187103675728.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemas:<br>\n",
    "• En primer lugar, se enfrentaría con el complicado problema de gradientes de fuga (vanishing gradients) (o el relacionado problema de gradientes de explosión) que afecta a las redes neuronales profundas y hace que las capas inferiores sean muy difíciles de entrenar.<br>\n",
    "• Segundo, con una red tan grande, la capacitación sería extremadamente lenta.<br>\n",
    "• En tercer lugar, un modelo con millones de parámetros podría correr el riesgo de sobreajustar el conjunto de entrenamiento (overfitting).<br>\n",
    "En este capítulo, analizaremos cada uno de estos problemas y presentaremos técnicas para resolverlos. Comenzaremos explicando el problema de la degradación de los gradientes y explorando algunas de las soluciones más populares para este problema. A continuación, veremos varios optimizadores que pueden acelerar el entrenamiento de grandes modelos enormemente en comparación con el Descenso de gradiente simple. Finalmente, veremos algunas técnicas de regularización populares para grandes redes neuronales.\n",
    "Con estas herramientas, podrás entrenar redes muy profundas: ¡bienvenido a Deep Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problemas de desvanecimiento / explosión de gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos anteriormente, el algoritmo de propagación hacia atrás (backpropagtion) funciona al pasar de la capa de salida a la capa de entrada, propagando el gradiente de error en el camino. Una vez que el algoritmo ha calculado el gradiente de la función de costo con respecto a cada parámetro en la red, utiliza estos gradientes para actualizar cada parámetro con un paso de Descenso de gradiente.\n",
    "Desafortunadamente, los gradientes a menudo se hacen cada vez más pequeños a medida que el algoritmo avanza hacia las capas inferiores. Como resultado, la actualización de Gradient Descent deja los pesos de conexión de la capa inferior prácticamente sin cambios, y el entrenamiento nunca converge en una buena solución. Esto se llama el problema de gradientes de fuga. En algunos casos, puede suceder lo contrario: los gradientes pueden crecer más y más, por lo que muchas capas obtienen actualizaciones de peso increíblemente grandes y el algoritmo diverge. <b>Este es el problema de los gradientes de explosión, que se encuentra principalmente en las redes neuronales recurrentes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más generalmente, las redes neuronales profundas sufren de gradientes inestables; Las diferentes capas pueden aprender a velocidades muy diferentes.\n",
    "Aunque este comportamiento desafortunado ha sido observado empíricamente durante bastante tiempo (fue una de las razones por las que las redes neuronales profundas fueron abandonadas durante mucho tiempo), solo alrededor de 2010 se logró un progreso significativo en su comprensión. Un artículo titulado \"Entendiendo la dificultad de entrenar redes neuronales profundas de alimentación hacia adelante(feedforward)\" por Xavier Glorot y Yoshua Bengio encontró algunos sospechosos, incluida la combinación de la popular función logística de activación sigmoidea y la técnica de inicialización de peso que era más popular en ese momento, a saber,inicialización  aleatoria utilizando una distribución normal con una media de 0 y una desviación estándar de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, mostraron que con esta función de activación y este esquema de inicialización, la varianza de las salidas de cada capa es mucho mayor que la varianza de sus entradas. Al avanzar en la red, la variación continúa aumentando después de cada capa hasta que la función de activación se satura en las capas superiores. En realidad, esto se agrava por el hecho de que la función logística tiene una media de 0,5, no 0 (la función tangente hiperbólica tiene una media de 0 y se comporta ligeramente mejor que la función logística en redes profundas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar la función de activación logística (ver Figura 11-1), puede ver que cuando las entradas se vuelven grandes (negativa o positiva), la función se satura en 0 o 1, con una derivada muy cerca de 0. Por lo tanto, cuando la propagación hacia atrás entra en acción, prácticamente no tiene gradiente para propagarse a través de la red, y el poco gradiente que existe se diluye a medida que la propagación hacia atrás progresa hacia abajo a través de las capas superiores, por lo que realmente no queda nada para las capas más bajas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de Xavier y He.\n",
    "<img src=\"https://previews.123rf.com/images/trueffelpix/trueffelpix1512/trueffelpix151200014/50242602-pensando-el-hombre-de-negocios-la-soluci%C3%B3n-de-un-problema.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su artículo, Glorot y Bengio proponen una manera de aliviar significativamente este problema.\n",
    "<b>Necesitamos que la señal fluya correctamente en ambas direcciones: en la dirección hacia adelante al hacer predicciones, y en la dirección contraria cuando se propagan hacia atrás los gradientes.\n",
    "No queremos que la señal se apague, ni queremos que explote y se sature.\n",
    "Para que la señal fluya correctamente, los autores argumentan que necesitamos que la varianza de las salidas de cada capa sea igual a la varianza de sus entradas, y también necesitamos que los gradientes tengan la misma varianza antes y después de fluir a través de una capa en la dirección inversa (consulte el documento si está interesado en los detalles matemáticos).</b> En realidad, no es posible garantizar ambas a menos que la capa tenga un número igual de conexiones de entrada y salida, pero propusieron un buen compromiso que ha demostrado funcionar muy bien en la práctica: los pesos de conexión deben inicializarse al azar como se describe en la Ecuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde ninputs y noutputs son el número de conexiones de entrada y salida para la capa cuyos pesos se están inicializando (también llamados entrada y salida de ventilador). Esta estrategia de inicialización a menudo se llama inicialización de Xavier (después del primer nombre del autor) o, en ocasiones, inicialización de Glorot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de la estrategia de inicialización de Xavier puede acelerar considerablemente el entrenamiento, y es uno de los trucos que llevaron al éxito actual de Deep Learning. Algunos artículos recientes han proporcionado estrategias similares para diferentes funciones de activación, como se muestra en la Tabla.  La estrategia de inicialización para la función de activación ReLU (y sus variantes, incluida la activación ELU descrita brevemente) a veces se denomina inicialización He (después del apellido de su autor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialization parameters for each type of activation function\n",
    "Iniciliazacion de Xavier (para dis normal la media es cero) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.programmersought.com/images/61/ddd08eb25ed616867524088228d9cfad.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma predeterminada, la función fully_connected () (introducida en el Capítulo 10) utiliza la inicialización de Xavier (con una distribución uniforme). Puede cambiar esto a la inicialización de He usando la función variance_scaling_initializer () de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = fully_connected(X, n_hidden1, weights_initializer=he_init, scope=\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Funciones de activación no saturantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las ideas en el documento de 2010 de Glorot y Bengio fue que los problemas de gradiente de fuga / explosión se debieron en parte a una mala elección de la función de activación.\n",
    "Hasta entonces, la mayoría de las personas había asumido que si la Madre Naturaleza hubiera elegido utilizar funciones de activación aproximadamente sigmoideas en las neuronas biológicas, debía ser una excelente opción. Pero resulta que otras funciones de activación se comportan mucho mejor en redes neuronales profundas, en particular la función de activación ReLU, principalmente porque no se satura para valores positivos (y también porque es bastante rápido de calcular).\n",
    "Desafortunadamente, la función de activación ReLU no es perfecta. Sufre de un problema conocido como las ReLU moribundas: durante el entrenamiento, algunas neuronas mueren de manera efectiva, lo que significa que dejan de emitir cualquier otra cosa que no sea 0. En algunos casos, es posible que la mitad de las neuronas de su red estén muertas, especialmente si utiliza una gran cantidad de neuronas. tasa de aprendizaje. Durante el entrenamiento, si los pesos de una neurona se actualizan de manera tal que la suma ponderada de las entradas de la neurona sea negativa, comenzará a emitir 0. Cuando esto suceda, es poco probable que la neurona vuelva a la vida, ya que el gradiente de la función ReLU es 0 cuando Su entrada es negativa.\n",
    "<b>Para resolver este problema, es posible que desee utilizar una variante de la función ReLU, como la ReLU con fugas. Esta función se define como LeakyReLUα (z) = max (αz, z). El hiperparámetro a define la cantidad de \"fugas\" de la función: es la pendiente de la función para z <0, y generalmente se establece en 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1000/0*rEvdFGrVWP2Tp_Kw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta pequeña pendiente asegura que las ReLU con fugas nunca mueran; Pueden entrar en un coma largo, pero tienen la oportunidad de despertarse. Un artículo reciente5 comparó varias variantes de la función de activación de ReLU y una de sus conclusiones fue que las variantes con fugas siempre superaron la función de activación de ReLU estricta. De hecho, la configuración de α = 0.2 (fuga enorme) parece dar como resultado un mejor rendimiento que α = 0.01 (fuga pequeña). También evaluaron la ReLU con fugas aleatorias (RReLU), donde α se selecciona aleatoriamente en un rango dado durante el entrenamiento, y se fija en un valor promedio durante la prueba. También funcionó bastante bien y pareció actuar como un organizador (reduciendo el riesgo de un ajuste excesivo del conjunto de entrenamiento).\n",
    "Finalmente, también evaluaron la ReLU con pérdidas paramétricas (PReLU), donde α está autorizada para aprender durante el entrenamiento (en lugar de ser un hiperparámetro, se convierte en un parámetro que puede modificarse mediante la propagación hacia atrás como cualquier otro parámetro). Esta se informó que superaba con creces a ReLU en conjuntos de datos de imágenes grandes, pero en conjuntos de datos más pequeños corre el riesgo de que se adapte en exceso al conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, pero no menos importante, un artículo de 2015 de Djork-Arné Clevert et al.6 propuso una nueva función de activación llamada unidad lineal exponencial (ELU) que superó todas las variantes de ReLU en sus experimentos: el tiempo de entrenamiento se redujo y la red neuronal se desempeñó mejor en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1192/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parece mucho a la función ReLU, con algunas diferencias importantes:<br>\n",
    "• Primero toma valores negativos cuando z <0, lo que permite que la unidad tenga una salida promedio más cercana a 0. Esto ayuda a aliviar el problema de los gradientes de fuga, como se explicó anteriormente. El hiperparámetro α define el valor al que se acerca la función ELU cuando z es un número negativo grande. Por lo general, se establece en 1, pero puede modificarlo como cualquier otro hiperparámetro si lo desea.<br>\n",
    "• Segundo, tiene un gradiente distinto de cero para z <0, lo que evita el problema de las unidades moribundas.<br>\n",
    "• Tercero, la función es suave en todas partes, incluso alrededor de z = 0, lo que ayuda a acelerar el Descenso de gradiente, ya que no rebota tanto a la izquierda como a la derecha de z = 0.<br>\n",
    "El principal inconveniente de la función de activación de ELU es que es más lento de computar que la ReLU y sus variantes (debido al uso de la función exponencial), pero durante el entrenamiento esto se compensa con la mayor velocidad de convergencia. Sin embargo, en el momento de la prueba, una red ELU será más lenta que una red ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, ¿qué función de activación debería usar para las capas ocultas de sus redes neuronales profundas? Aunque su millaje variará, en general ELU> ReLU con fugas (y sus variantes)> ReLU> tanh> logistic.\n",
    "Si le preocupa mucho el rendimiento en tiempo de ejecución, entonces puede que prefiera ReLU con pérdidas sobre las ELU. Si no desea ajustar otro hiperparámetro, puede usar los valores α predeterminados sugeridos anteriormente (0.01 para la ReLU con fugas y 1 para ELU). Si tiene tiempo y capacidad de computación libres, puede usar la validación cruzada para evaluar otras funciones de activación, en particular RReLU si su red está sobrecargada, o PReLU si tiene un gran conjunto de capacitación.\n",
    "TensorFlow ofrece una función elu () que puede usar para construir su red neuronal.\n",
    "Simplemente establezca el argumento activation_fn al llamar a la función fully_connected (), de esta manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow no tiene una función predefinida para ReLU con fugas, pero es bastante fácil de definir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalización de lotes (Batch normalization)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el uso de la inicialización de He junto con ELU (o cualquier variante de ReLU) puede reducir significativamente los problemas de gradiente de fuga / explosión al comienzo de la capacitación, no garantiza que no volverán durante la capacitación.\n",
    "En un documento de 2015, 7 Sergey Ioffe y Christian Szegedy propusieron una técnica llamada Batch Normalization (BN) para abordar los problemas de gradientes de fuga / explosión, y más generalmente el problema de que la distribución de las entradas de cada capa cambia durante el entrenamiento, como los parámetros del Las capas anteriores cambian (lo que llaman el problema del Cambio de Covariate Interno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La técnica consiste en agregar una operación en el modelo justo antes de la función de activación de cada capa, simplemente centrar en cero y normalizar las entradas, luego escalar y cambiar el resultado utilizando dos nuevos parámetros por capa (uno para el escalado y otro para el cambio) . En otras palabras, esta operación le permite al modelo aprender la escala óptima y la media de las entradas para cada capa.\n",
    "Para centrar en cero y normalizar las entradas, el algoritmo debe estimar la media y la desviación estándar de las entradas. Lo hace mediante la evaluación de la media y la desviación estándar de las entradas sobre el mini lote actual (de ahí el nombre \"Normalización por lotes\").\n",
    "Toda la operación se resume en la ecuación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1153/1*xQhPvRh08oKFC63swgWr_w.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• μB es la media empírica, evaluada sobre todo el mini lote B.<br>\n",
    "• σB es la desviación estándar empírica, también evaluada en todo el mini-lote.<br>\n",
    "• m es el número de instancias en el mini-lote.<br>\n",
    "• ^X(i) es la entrada normalizada y centrada en cero.<br>\n",
    "• γ es el parámetro de escala para la capa.<br>\n",
    "• β es el parámetro de desplazamiento (desplazamiento) para la capa.<br>\n",
    "• ϵ es un número pequeño para evitar la división por cero (normalmente 10–3). Esto se llama un término de suavizado.<br>\n",
    "• y (i) es la salida de la operación BN: es una versión escalada y modificada de las entradas.<br>\n",
    "En el momento de la prueba (test), no hay un mini lote para calcular la media empírica y la desviación estándar, por lo que, en lugar de eso, simplemente use la media y la desviación estándar de todo el conjunto de entrenamiento.<br>\n",
    "Estos se suelen calcular de manera eficiente durante el entrenamiento utilizando una media móvil.\n",
    "Entonces, en total, se aprenden cuatro parámetros para cada capa normalizada por lotes: γ (escala), β (offset), μ (media) y σ (desviación estándar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autores demostraron que esta técnica mejoró considerablemente todas las redes neuronales profundas con las que experimentaron. El problema de los gradientes de fuga se redujo considerablemente, hasta el punto de que podían utilizar funciones de activación de saturación como el tanh e incluso la función de activación logística. Las redes también fueron mucho menos sensibles a la inicialización del peso. Pudieron utilizar tasas de aprendizaje mucho más altas, acelerando significativamente el proceso de aprendizaje. Específicamente, señalan que “Aplicada a un modelo de clasificación de imágenes de vanguardia, la Normalización de lotes logra la misma precisión con 14 veces menos pasos de entrenamiento, y supera al modelo original por un margen significativo. […] Usando un conjunto de redes normalizadas por lotes, mejoramos el mejor resultado publicado en la clasificación de ImageNet: alcanzamos el 4.9% de error de validación de top 5 (y 4.8% de error de prueba), superando la precisión de los evaluadores humanos. \"Finalmente, como un regalo que sigue dando, la normalización de lotes también actúa como un regularizador, reduciendo la necesidad de otras técnicas de regularización (como el abandono (dropout), que se describe más adelante en este capítulo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, la normalización de lotes agrega cierta complejidad al modelo (aunque elimina la necesidad de normalizar los datos de entrada ya que la primera capa oculta se encargará de eso, siempre que se normalice por lotes). Además, hay una penalización de tiempo de ejecución: la red neuronal hace predicciones más lentas debido a los cálculos adicionales requeridos en cada capa. Por lo tanto, si necesita que las predicciones sean rápidas, le recomendamos que compruebe qué tan bien se realiza la inicialización de ELU + He antes de jugar con la Normalización de lotes.\n",
    "Puede encontrar que el entrenamiento es bastante lento al principio, mientras que Gradient Descent está buscando las escalas y compensaciones óptimas para cada capa, pero se acelera una vez que encuentra valores razonablemente buenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementando la normalización de lotes con TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow proporciona una función batch_normalization () que simplemente centra y normaliza las entradas, pero usted mismo debe calcular la media y la desviación estándar (basándose en los datos de mini lotes durante el entrenamiento o en el conjunto de datos completo durante las pruebas, como se acaba de explicar) y pasarlos. como parámetros para esta función, y también debe manejar la creación de los parámetros de escalado y desplazamiento (y pasarlos a esta función). Es factible, pero no es el enfoque más conveniente. En su lugar, debes usar la función batch_norm (), que maneja todo esto por ti. Puede llamarlo directamente o decirle a la función fully_connected () que lo use, como en el siguiente código: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fully_connected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dbbab444e11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'is_training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbn_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'is_training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'updates_collections'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\",\n\u001b[0m\u001b[1;32m     11\u001b[0m normalizer_fn=batch_norm, normalizer_params=bn_params)\n\u001b[1;32m     12\u001b[0m hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\",\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fully_connected' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "bn_params = {'is_training': is_training,'decay': 0.99,'updates_collections': None}\n",
    "hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\",\n",
    "normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\",\n",
    "normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "logits = fully_connected(hidden2, n_outputs, activation_fn=None,scope=\"outputs\",normalizer_fn=batch_norm, normalizer_params=bn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
